\documentclass{amsproc}

%------------------------------
%     PREAMBLE
%------------------------------

% Packages
\usepackage{amsmath, amsthm, hyperref, mathtools} 
\usepackage[usenames, dvipsnames]{xcolor}
\usepackage{bibentry}
\usepackage{lipsum}

% Hyperlink package parameters
\hypersetup{
	colorlinks=true,
	linkcolor=RoyalBlue,
	citecolor=Thistle
}

% amsthm - Declaring theorem commands
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{remark}[theorem]{Remark}

% Referencing including the format name (e.g. 'Section', 'Theorem') not just the numbering
\newcommand{\fref}[2]{\hyperref[#2]{#1 \ref*{#2}}}

% SHORTCUTS
% Notation (general)
\newcommand{\defeq}{\vcentcolon=} % :=
\newcommand{\eqdef}{=\vcentcolon} % =:
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Rn}{\mathbb{R}^n}
% Notation (probability)
\renewcommand{\P}{\mathbb{P}} % Probability
\newcommand{\I}{\mathbb{I}} % Indicator
\newcommand{\E}{\mathbb{E}} % Expectation
\newcommand{\F}{\mathcal{F}} % Filtration
\newcommand{\var}{\mathbb{VAR}} % Variance
\DeclareMathOperator{\Ent}{Ent} % Entropy

%------------------------------
%     DOCUMENT
%------------------------------

% Title
\title{Concentration of measure}
\author{SIAM Working Group - Spring 2019}

\begin{document}

% Title and table of contents
\maketitle
\tableofcontents

%------------------------------
%     Lecture 1
%------------------------------

\section{Introduction}
\label{sec:introduction}
\paragraph{\textbf{Abstract}}
	In this lecture, we introduce the book's central theme, the study of random fluctuations of functions of independent random variables, along with three techniques that facilitate this study. The specific focus is on how these functions concentrate around measures of central tendency such as their mean and median. We discuss the three main methods below.

\subsection*{Method 1: Isoperimetric Inequalities}
\label{sec:method_of_isoperimetric_inequalities}
	Suppose $(\mathcal{X},d)$ is a metric space, $X$ is a $\mathcal{X}$-valued RV with law $\P$, and $Mf(X)$ is a median of $f(X)$. Then
	\[
		\P\left\{|f(X)-Mf(X)|\geq t\right\}\leq 2\alpha(t)
	\]
	where
	\[
		\alpha(t)\defeq\sup_{A\in \mathcal{B}(\mathcal{X})\atop \P(A)\geq\frac{1}{2}}\P\{d(X,A)\geq t\}
	\]
	and $\mathcal{B}(\mathcal{X})$ is the Borel $\sigma$-algebra on $\mathcal{X}$. Thus, bounding $\alpha(t)$ allows one to describe how $f(X)$ concentrates around its median.

\subsection*{Method 2: Entropy}
\label{sec:entropy_method}
	Let $I\subseteq\R$ be an interval, $X$ a $I$-valued RV, and $\Phi:I\to\R$ a convex function. then the $\Phi$-entropy of $X$ is
	\[
		H_\Phi(X)\defeq E\Phi(X)-\Phi(EX).
	\]
	If $\Phi(x)=x\log(x)$ then we write $\text{Ent}$ in place of $H_\Phi$. Bounds on this entropy can be translated to bounds on the concentration of functions of random variables around their mean. For example the Gaussian logarithmic Sobolev inequality states that if $X\sim N(0,I_{n\times n})$ and $f\in C^1(\Rn)$ then
	\[
		\text{Ent}\left[f^2(X)\right]\leq 2E\left[\|\nabla f(X)\|^2\right].
	\]
	In turn, this implies that
	\[
		\P\left\{f(X)-Ef(X)\geq t\right\}\leq e^{-\frac{t^2}{2}}.
	\]

\subsection*{Method 3: Transportation}
\label{sec:transport_method}
	Another way of deriving concentration bounds is by getting estimates of the \textit{transportation cost} between two probability measures $P$ and $Q$:
	\[
		\min_{\mathbf{P}\in\mathcal{P}(P,Q)} \E_\mathbf{P}d(X,Y)
	\]
	where $d$ is some cost function, and $\mathcal{P}(P,Q)$ is the class of joint distributions of $X$ and $Y$ such that the marginal distribution of $X$ is $P$ and the marginal distribution of $Y$ is $Q$.

%------------------------------
%     Lecture 2
%------------------------------

\section{Basic inequalities}
\label{sec:basic_inequalites}
\paragraph{\textbf{Abstract}}
	We use the Cram\'{e}r-Chernoff method to establish a family of inequalities which give quantitative estimates on concentration of measure subject to certain regularity conditions. We use the machinery of the Cram\'{e}r-Chernoff method, the log of the moment generating function, to define families of random variables with rapidly decaying tails, sub-Gaussian and more generally sub-Gamma random variables.

\begin{theorem}[Hoeffdinig's Inequality]
\label{thm:hoeffding}
	Let $X_1,\, \dots,\, X_n$ be independent random variables such that $X_i$ takes its values in $[a_i, b_i]$ almost surely for all $i \leq n$. Let
	\[S \defeq \sum_{i=1}^n (X_i - \E X_i).\]
	Then for every $t > 0$,
	\[\P \{S \geq t \} \leq  \exp \left( \frac{-2t^2}{\sum_{i=1}^n (b_i-a_i)^2} \right).\]
\end{theorem}

\begin{remark}[Cram\'{e}r-Chernoff Method]
\label{rmk:cramer_chernoff}
	For a random variable $X$, we use the fact that $\exp(\cdot)$ is an increasing function along with Markov's inequality to get the estimate
	\[\P \{X \geq t \} = \P \{e^{\lambda X} \geq e^{\lambda t} \} \leq e^{-\lambda t} \E \left( e^{\lambda X} \right)\]
	for every $\lambda > 0$.
	Picking the optimal $\lambda$ for a fixed $t$ yields
	\[\P \{X \geq t \} \leq \exp \left( -\psi^*(t) \right) \]
	where 
	\[\psi(\lambda) \defeq \log \E \left( e^{\lambda X} \right) \text{ and}\]
	\[\psi^*(t) \defeq \sup_{\lambda > 0} \left( t \lambda - \psi(\lambda) \right) \]
	which agrees with the Legendre transform of $\psi$ when $t > \E X$.
\end{remark}

%------------------------------
%     Lecture 3
%------------------------------

\section{Bounding the variance}
\label{sec:bound_var}
\paragraph{\textbf{Abstract}}
We extensively utilize the \textit{Efron-Stein} inequality, which gives a bound on the variance of random variables in terms of the mean of their conditional variances. This simple bound has surprising consequences when applied to specific types of functions of random variables and allows us to almost effortlessly obtain sharp bounds on the variance. This is particularly the case for functions satisfying the \textit{bounded difference}, \emph{self-bounding} and \emph{configuration} properties. Other applications include proving Poincar\'{e} type inequalities for Gaussian RV's and deriving exponential tail bounds.

\begin{theorem}[Efron-Stein Inequality]
\label{thm:ESI}
	Let $X_1,\,\dots,\,X_n$ be independent random variables taking values in $\mathcal{X}$ and let $f: \mathcal{X}^n \to \R$ be given.
	Set $Z = f(X_1,\,\dots,\,X_n)$ and assume $Z \in L^2$. Then we have that 
	$$\var(Z)\leq \sum\limits_{i=1}^n \E[(Z-\E^{(i)}[Z])^2] \eqdef \sum\limits_{i=1}^n \E[\var^{(i)}[Z]]$$ where 
	$$\E^{(i)}[Z] \defeq \E[Z|X_1,\,\dots,\,X_{i-1},\,X_{i+1},\,\dots,\,X_n]$$ and $$\var^{i}[Z] \defeq \E^{(i)}[(Z - \E^{(i)}[Z])^2].$$ 
\end{theorem}

\begin{remark}[Fubini Trick]
\label{rmk:Fubini}
	If we write $Z = f(X_1,\,\dots,\,X_n)$ as before and 
	$\E_i[Z] \defeq \E[Z|X_1,\,\dots,\,X_i]$ then by Fubini we have that 
	$$\E_i[Z] = \int_{\mathcal{X}^{n-i}} f(X_1,\,\dots,\,X_i,\,x_{i+1},\,\dots,\,x_{n})d\mu_{i+1}(x_{i+1})\dots d\mu_n(x_{n})$$ and similarly
	$$\E^{(i)}[Z] = \int_\mathcal{X} f(X_1,\,\dots,\,X_{i-1},\,x_i,\,X_{i+1},\,\dots,\,X_n)d\mu_i(x_i)$$ so that we have the identity
	$$\E_i[\E^{(i)}[Z]] = \E_{i-1}[Z], \hspace{0.3 cm} \forall i \geq 1$$
	where we use the convention that $\E_0[Z] = \E[Z]$. Now if we let $\Delta_i \defeq \E_i[Z] - \E_{i-1}[Z]$ then by this Fubini observation it follows that 
	$$Z - \E[Z] = \sum\limits_{i=1}^n \Delta_i = \sum\limits_{i=1}^n \E^i[Z - E^{(i)}[Z]]$$ 
	Since for $j > i$ we have that
	$$\E[\Delta_i\Delta_j] = \E[\Delta_i\E_i[\Delta_j]] = 0$$ it follows that, using Jensen's inequality,
	\begin{align*}
		\var(Z)
		= \E[(Z-\E[Z])^2]
		= \sum\limits_{i=1}^n \E[\Delta_i^2] + 2\sum\limits_{j>i}\E[\Delta_i\Delta_j]
		&= \sum\limits_{i=1}^n \E[(\E_{i=1}[Z - \E^{i}[Z]])^2]\\
		&\leq\sum\limits_{i=1}^n \E[(Z - \E^{(i)}[Z])^2]
	\end{align*}
	which gives us Efron-Stein.  
\end{remark}

%------------------------------
%     Lecture 4
%------------------------------

\section{Basic information inequalities}
\label{sec:bas_inf_ineq}
\paragraph{\textbf{Abstract}}
	In this lecture we lay the groundwork for the `entropy method'.
	In particular we discuss basic properties of the Shannon entropy (relation to conditioning and `Chain Rule'),
	we sketch the proofs of Han's inequality and its variant for relative entropies,
	and we prove sub-additivity of the entropy for discrete random variables
	(which is an exact analogue of the Efron-Stein inequality where entropies replace variances).

\begin{theorem}[Sub-Additivity of Entropy]
\label{thm:sub_add_entropy}
	Let $\Phi(x) \defeq x\log x$ for $x>0$, let $X_1,\,\dots,\,X_n$ be independent random variables, and
	let $Y = f(X_1,\,\dots,\,X_n)$ be a nonnegative measurable function of these variables such that $\Phi(Y) = Y\log Y$ is integrable.

	For every $1\leq i \leq n$, denote by 
	\begin{itemize}
		\item	$\E^{(i)}$ the expectation operator conditioned on $$X^{(i)} = (X_1,\,\dots,\,X_{i-1},\,X_{i+1},\,\dots,\,X_n),$$
			i.e. $\E^{(i)} [\,\cdot\,] \defeq \E[\,\cdot\, | X_1,\,\dots,\,X_{i-1},\,X_{i+1},\,\dots,\,X_n],$
		\item	$\Ent (Y)$ the \emph{entropy} of $Y$ defined by $$\Ent (Y) \defeq \E\Phi (Y) - \Phi(\E Y),$$ and
		\item	$\Ent^{(i)} (Y)$ the \emph{conditional entropy} of $Y$ given $X^{(i)}$
			defined by $$\Ent^{(i)} \defeq \E^{(i)}\Phi (Y) - \Phi(\E^{(i)} Y).$$
	\end{itemize}

	Then $$\Ent (Y) \leq \E \sum_{i=1}^n \Ent^{(i)} (Y).$$
\end{theorem}

\begin{remark}[Entropy as a Kullback-Leibler divergence (discrete case)]
\label{rmk:entropy_as_KL_divergence}
	For any discrete probability distributions $P$ and $Q$, if $P$ is absolutely continuous with respect to $Q$ then the Kullback-Leibler divergence from $P$ to $Q$ is defined as
	$$D(P||Q) \defeq \sum_x p(x) \log \frac{p(x)}{q(x)}.$$
	Recall that this can be interpreted as the additional string length required (on average) to encode a string generated by $P$ using a code designed for $Q$.
	This is why $P$ and $Q$ are sometimes referred to as the `reference' and `target' distributions respectively.

	Now consider a random variable $Z=f(X)$ such that $\E Z=1$, let $P$ be the disribution of $X$, and let $Q$ be defined via $q(x) = f(x) p(x)$.
	Then $$D(Q||P) = \Ent Z.$$
	Indeed $$D(Q||P) = \sum_x f(x) p(x) \frac{f(x) p(x)}{p(x)} = \sum_x f(x) p(x) \log f(x) = \E Z Log Z = \Ent Z$$
	since $\Phi(\E Z) = \Phi(1) = 0$.
\end{remark}

%------------------------------
%     APPENDIX
%------------------------------

\appendix
\newpage
\section*{List of results}
	\begin{tabular}{l}
		\hyperref[thm:hoeffding]{Hoeffding's inequality}\\
		\hyperref[thm:ESI]{Efron-Stein inequality}\\
		\hyperref[thm:sub_add_entropy]{Sub-additivity of entropy}\\
	\end{tabular}
\section*{List of `tricks'}
	\begin{tabular}{l}
		\hyperref[sec:method_of_isoperimetric_inequalities]{Method of isoperimetric inequalities}\\
		\hyperref[sec:entropy_method]{Entropy method}\\
		\hyperref[sec:transport_method]{Transport method}\\
		\hyperref[rmk:cramer_chernoff]{Cram\'{e}r-Chernoff method}\\
		\hyperref[rmk:Fubini]{Fubini trick}\\
		\hyperref[rmk:entropy_as_KL_divergence]{Entropy as a Kullback-Leibler divergence (discrete case)}
	\end{tabular}
\section*{Credits}
	\begin{tabular}{ll}
		\fref{Lecture}{sec:introduction}& David Gutman\\
		\fref{Lecture}{sec:basic_inequalites}& Adrian Hagerty\\
		\fref{Lecture}{sec:bound_var}& David Itkin\\
		\fref{Lecture}{sec:bas_inf_ineq}& Antoine Remond-Tiedrez
	\end{tabular}

\bibliographystyle{alpha}
\bibliography{ref}
\bibentry{boucheron_lugosi_massart}

\end{document}
