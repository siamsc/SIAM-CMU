\documentclass{amsproc}

%------------------------------
%     PREAMBLE
%------------------------------

% Packages
\usepackage{amsmath, amsthm, hyperref} 
\usepackage[usenames, dvipsnames]{xcolor}
\usepackage{bibentry}
\usepackage{lipsum}

% Hyperlink package parameters
\hypersetup{
	colorlinks=true,
	linkcolor=RoyalBlue,
	citecolor=Thistle
}

% amsthm - Declaring theorem commands
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{remark}[theorem]{Remark}

% Referencing including the format name (e.g. 'Section', 'Theorem') not just the numbering
\newcommand{\fref}[2]{\hyperref[#2]{#1 \ref*{#2}}}

%------------------------------
%     DOCUMENT
%------------------------------

% Title
\title{Concentration of measure}
\author{SIAM Working Group - Spring 2019}

\begin{document}

% Title and table of contents
\maketitle
\tableofcontents

%------------------------------
%     Lecture 1
%------------------------------

\section{Introduction}
\label{sec:introduction}
	\paragraph{\textbf{Abstract}}
	In this lecture, we introduced the book's central theme, the study of random fluctuations of functions of independent random variables, along with three techniques that facilitate this study. The specific focus is on how these functions concentrate around measures of central tendency such as their mean and median. We discuss the three main methods below.

	\subsection*{Method 1: Isoperimetric Inequalities}

	Suppose $(\mathcal{X},d)$ is a metric space, $X$ is a $\mathcal{X}$-valued RV with law $P$, and $Mf(X)$ is a median of $f(X)$. Then
	\[
		P\left\{|f(X)-Mf(X)|\geq t\right\}\leq 2\alpha(t)
	\]
	where
	\[
		\alpha(t):=\sup_{A\in \mathcal{B}(\mathcal{X})\atop P(A)\geq\frac{1}{2}}P\{d(X,A)\geq t\}
	\]
	and $\mathcal{B}(\mathcal{X})$ is the Borel $\sigma$-algebra on $\mathcal{X}$. Thus, bounding $\alpha(t)$ allows one to describe how $f(X)$ concentrates around its median.

	\subsection*{Method 2: Entropy}

	Let $I\subseteq\mathbb{R}$ be an interval, $X$ a $I$-valued RV, and $\Phi:I\to\mathbb{R}$ a convex function. then the $\Phi$-entropy of $X$ is
	\[
		H_\Phi(X):=E\Phi(X)-\Phi(EX).
	\]
	If $\Phi(x)=x\log(x)$ then we write $\text{Ent}$ in place of $H_\Phi$. Bounds on this entropy can translated to bounds on the concentration of functions of random variables around their mean. For example the Gaussian logarithmic Sobolev inequality states that if $X\sim N(0,I_{n\times n})$ and $f\in C^1(\mathbb{R}^n)$ then
	\[
		\text{Ent}\left[f^2(X)\right]\leq 2E\left[\|\nabla f(X)\|^2\right].
	\]
	In turn, this implies that
	\[
		P\left\{f(X)-Ef(X)\geq t\right\}\leq e^{-\frac{t^2}{2}}
	\]
	\subsection*{Method 3: Transportation}

	One other way of deriving concentration bounds is by getting estimates of the \textit{transportation cost} between two probability measures $P$ and $Q$:
	\[
		\min_{\mathbf{P}\in\mathcal{P}(P,Q)}E_\mathbf{P}d(X,Y)
	\]
	where $d$ is some cost function, and $\mathcal{P}(P,Q)$ is the class of joint distributions of $X$ and $Y$ s.t. the marginal distribution of $X$ is $P$ and the marginal distribution of $Y$ is $Q$.

%------------------------------
%     Lecture 2
%------------------------------

\section{Basic inequalities}
\label{sec:basic_inequalites}
\paragraph{\textbf{Abstract}}
We used the Cram\'{e}r-Chernoff method to establish a family of inequalities which give quantitative estimates on concentration of measure in the case of certain regularity conditions. We use the machinery of the Cram\'{e}r-Chernoff method, the log of the moment generating function, to define families of random variables with rapidly decaying tails, sub-Gaussian and more generally sub-Gamma random variables.

\begin{theorem}[Hoeffdinig's Inequality]
\label{thm:hoeffding}
Let $X_1, \dots, X_n$ be independent random variables such that $X_i$ takes its values in $[a_i, b_i]$ almost surely for all $i \leq n$. Let
\[S = \sum_{i=1}^n (X_i - \mathbf{E}X_i).\]
Then for every $t > 0$,
\[\mathbf{P} \{S \geq t \} \leq  \exp \left( \frac{-2t^2}{\sum_{i=1}^n (b_i-a_i)^2} \right).\]
\end{theorem}

\begin{remark}[Cram\'{e}r-Chernoff Method]
\label{rmk:cramer-chernoff}
For a random variable $X$, we use the fact that $\exp(\cdot)$ is an increasing function along with Markov's inequality to get the estimate
\[\mathbf{P} \{X \geq t \} = \mathbf{P} \{e^{\lambda X} \geq e^{\lambda t} \} \leq e^{-\lambda t} \mathbf{E}\left( e^{\lambda X} \right)\]
for every $\lambda > 0$.
Picking the optimal $\lambda$ for a fixed $t$ yields
\[\mathbf{P} \{X \geq t \} \leq \exp \left( -\psi^*(t) \right) \]
where 
\[\psi(\lambda) = \log \mathbf{E}\left( e^{\lambda X} \right)\]
\[\psi^*(t) = \sup_{\lambda > 0} \left( t \lambda - \psi(\lambda) \right) \]
which agrees with the Legendre transform of $\psi$ when $t > \mathbf{E}X$.
\end{remark}

%------------------------------
%     Lecture 2
%------------------------------

\section{Bounding the variance}
\label{sec:bound_var}
\paragraph{\textbf{Abstract}}
% YOUR STUFF HERE

\begin{theorem}[Name of important result]
\label{thm:reference_name}
% YOUR STUFF HERE
\end{theorem}

\begin{remark}[Name of `trick']
\label{rmk:reference_name}
% YOUR STUFF HERE
\end{remark}

%------------------------------
%     APPENDIX
%------------------------------

\appendix
\newpage
\section*{Credits}
\begin{tabular}{ll}
	\fref{Lecture}{sec:introduction}
	&David Gutman
	\\
	\fref{Lecture}{sec:basic_inequalites}
	&Adrian Hagerty
\end{tabular}

\bibliographystyle{alpha}
\bibliography{ref}
\bibentry{boucheron_lugosi_massart}

\end{document}
