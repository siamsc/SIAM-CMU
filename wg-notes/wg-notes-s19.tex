\documentclass{amsproc}

%------------------------------
%     PREAMBLE
%------------------------------

% Packages
\usepackage{amsmath, amsthm, hyperref, mathtools} 
\usepackage[usenames, dvipsnames]{xcolor}
\usepackage{bibentry}
\usepackage{lipsum}

% Hyperlink package parameters
\hypersetup{
	colorlinks=true,
	linkcolor=RoyalBlue,
	citecolor=Thistle
}

% amsthm - Declaring theorem commands
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{remark}[theorem]{Remark}

% Referencing including the format name (e.g. 'Section', 'Theorem') not just the numbering
\newcommand{\fref}[2]{\hyperref[#2]{#1 \ref*{#2}}}

% SHORTCUTS
% Notation (general)
\newcommand{\defeq}{\vcentcolon=} % :=
\newcommand{\eqdef}{=\vcentcolon} % =:
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Rn}{\mathbb{R}^n}
% Notation (probability)
\renewcommand{\P}{\mathbb{P}} % Probability
\newcommand{\I}{\mathbb{I}} % Indicator
\newcommand{\E}{\mathbb{E}} % Expectation
\newcommand{\F}{\mathcal{F}} % Filtration
\newcommand{\var}{\mathbb{VAR}} % Variance
\DeclareMathOperator{\Ent}{Ent} % Entropy

%------------------------------
%     DOCUMENT
%------------------------------

% Title
\title{Concentration of measure}
\author{SIAM Working Group - Spring 2019}

\begin{document}

% Title and table of contents
\maketitle
\tableofcontents

%------------------------------
%     Lecture 1
%------------------------------

\section{Introduction}
\label{sec:introduction}
\paragraph{\textbf{Abstract}}
	In this lecture, we introduce the book's central theme, the study of random fluctuations of functions of independent random variables, along with three techniques that facilitate this study. The specific focus is on how these functions concentrate around measures of central tendency such as their mean and median. We discuss the three main methods below.

\subsection*{Method 1: Isoperimetric Inequalities}
\label{sec:method_of_isoperimetric_inequalities}
	Suppose $(\mathcal{X},d)$ is a metric space, $X$ is a $\mathcal{X}$-valued RV with law $\P$, and $Mf(X)$ is a median of $f(X)$. Then
	\[
		\P\left\{|f(X)-Mf(X)|\geq t\right\}\leq 2\alpha(t)
	\]
	where
	\[
		\alpha(t)\defeq\sup_{A\in \mathcal{B}(\mathcal{X})\atop \P(A)\geq\frac{1}{2}}\P\{d(X,A)\geq t\}
	\]
	and $\mathcal{B}(\mathcal{X})$ is the Borel $\sigma$-algebra on $\mathcal{X}$. Thus, bounding $\alpha(t)$ allows one to describe how $f(X)$ concentrates around its median.

\subsection*{Method 2: Entropy}
\label{sec:entropy_method}
	Let $I\subseteq\R$ be an interval, $X$ a $I$-valued RV, and $\Phi:I\to\R$ a convex function. then the $\Phi$-entropy of $X$ is
	\[
		H_\Phi(X)\defeq E\Phi(X)-\Phi(EX).
	\]
	If $\Phi(x)=x\log(x)$ then we write $\text{Ent}$ in place of $H_\Phi$. Bounds on this entropy can be translated to bounds on the concentration of functions of random variables around their mean. For example the Gaussian logarithmic Sobolev inequality states that if $X\sim N(0,I_{n\times n})$ and $f\in C^1(\Rn)$ then
	\[
		\text{Ent}\left[f^2(X)\right]\leq 2E\left[\|\nabla f(X)\|^2\right].
	\]
	In turn, this implies that
	\[
		\P\left\{f(X)-Ef(X)\geq t\right\}\leq e^{-\frac{t^2}{2}}.
	\]

\subsection*{Method 3: Transportation}
\label{sec:transport_method}
	Another way of deriving concentration bounds is by getting estimates of the \textit{transportation cost} between two probability measures $P$ and $Q$:
	\[
		\min_{\mathbf{P}\in\mathcal{P}(P,Q)} \E_\mathbf{P}d(X,Y)
	\]
	where $d$ is some cost function, and $\mathcal{P}(P,Q)$ is the class of joint distributions of $X$ and $Y$ such that the marginal distribution of $X$ is $P$ and the marginal distribution of $Y$ is $Q$.

%------------------------------
%     Lecture 2
%------------------------------

\section{Basic inequalities}
\label{sec:basic_inequalites}
\paragraph{\textbf{Abstract}}
	We use the Cram\'{e}r-Chernoff method to establish a family of inequalities which give quantitative estimates on concentration of measure subject to certain regularity conditions. We use the machinery of the Cram\'{e}r-Chernoff method, the log of the moment generating function, to define families of random variables with rapidly decaying tails, sub-Gaussian and more generally sub-Gamma random variables.

\begin{theorem}[Hoeffdinig's Inequality]
\label{thm:hoeffding}
	Let $X_1,\, \dots,\, X_n$ be independent random variables such that $X_i$ takes its values in $[a_i, b_i]$ almost surely for all $i \leq n$. Let
	\[S \defeq \sum_{i=1}^n (X_i - \E X_i).\]
	Then for every $t > 0$,
	\[\P \{S \geq t \} \leq  \exp \left( \frac{-2t^2}{\sum_{i=1}^n (b_i-a_i)^2} \right).\]
\end{theorem}

\paragraph{\textbf{Cram\'{e}-Chernoff Method}}
\label{rmk:cramer_chernoff}
	For a random variable $X$, we use the fact that $\exp(\cdot)$ is an increasing function along with Markov's inequality to get the estimate
	\[\P \{X \geq t \} = \P \{e^{\lambda X} \geq e^{\lambda t} \} \leq e^{-\lambda t} \E \left( e^{\lambda X} \right)\]
	for every $\lambda > 0$.
	Picking the optimal $\lambda$ for a fixed $t$ yields
	\[\P \{X \geq t \} \leq \exp \left( -\psi^*(t) \right) \]
	where 
	\[\psi(\lambda) \defeq \log \E \left( e^{\lambda X} \right) \text{ and}\]
	\[\psi^*(t) \defeq \sup_{\lambda > 0} \left( t \lambda - \psi(\lambda) \right) \]
	which agrees with the Legendre transform of $\psi$ when $t > \E X$.

%------------------------------
%     Lecture 3
%------------------------------

\section{Bounding the variance}
\label{sec:bound_var}
\paragraph{\textbf{Abstract}}
We extensively utilize the \textit{Efron-Stein} inequality, which gives a bound on the variance of random variables in terms of the mean of their conditional variances. This simple bound has surprising consequences when applied to specific types of functions of random variables and allows us to almost effortlessly obtain sharp bounds on the variance. This is particularly the case for functions satisfying the \textit{bounded difference}, \emph{self-bounding} and \emph{configuration} properties. Other applications include proving Poincar\'{e} type inequalities for Gaussian RV's and deriving exponential tail bounds.

\begin{theorem}[Efron-Stein Inequality]
\label{thm:ESI}
	Let $X_1,\,\dots,\,X_n$ be independent random variables taking values in $\mathcal{X}$ and let $f: \mathcal{X}^n \to \R$ be given.
	Set $Z = f(X_1,\,\dots,\,X_n)$ and assume $Z \in L^2$. Then we have that 
	$$\var(Z)\leq \sum\limits_{i=1}^n \E[(Z-\E^{(i)}[Z])^2] \eqdef \sum\limits_{i=1}^n \E[\var^{(i)}[Z]]$$ where 
	$$\E^{(i)}[Z] \defeq \E[Z|X_1,\,\dots,\,X_{i-1},\,X_{i+1},\,\dots,\,X_n]$$
	i.e. $\E^{(i)}$ the expectation operator conditioned on $$X^{(i)} = (X_1,\,\dots,\,X_{i-1},\,X_{i+1},\,\dots,\,X_n),$$
	and $$\var^{i}[Z] \defeq \E^{(i)}[(Z - \E^{(i)}[Z])^2].$$
\end{theorem}

\paragraph{\textbf{Fubini Trick}}
\label{rmk:Fubini}
	If we write $Z = f(X_1,\,\dots,\,X_n)$ as before and 
	$\E_i[Z] \defeq \E[Z|X_1,\,\dots,\,X_i]$ then by Fubini we have that 
	$$\E_i[Z] = \int_{\mathcal{X}^{n-i}} f(X_1,\,\dots,\,X_i,\,x_{i+1},\,\dots,\,x_{n})d\mu_{i+1}(x_{i+1})\dots d\mu_n(x_{n})$$ and similarly
	$$\E^{(i)}[Z] = \int_\mathcal{X} f(X_1,\,\dots,\,X_{i-1},\,x_i,\,X_{i+1},\,\dots,\,X_n)d\mu_i(x_i)$$ so that we have the identity
	$$\E_i[\E^{(i)}[Z]] = \E_{i-1}[Z], \hspace{0.3 cm} \forall i \geq 1$$
	where we use the convention that $\E_0[Z] = \E[Z]$. Now if we let $\Delta_i \defeq \E_i[Z] - \E_{i-1}[Z]$ then by this Fubini observation it follows that 
	$$Z - \E[Z] = \sum\limits_{i=1}^n \Delta_i = \sum\limits_{i=1}^n \E^i[Z - E^{(i)}[Z]]$$ 
	Since for $j > i$ we have that
	$$\E[\Delta_i\Delta_j] = \E[\Delta_i\E_i[\Delta_j]] = 0$$ it follows that, using Jensen's inequality,
	\begin{align*}
		\var(Z)
		= \E[(Z-\E[Z])^2]
		= \sum\limits_{i=1}^n \E[\Delta_i^2] + 2\sum\limits_{j>i}\E[\Delta_i\Delta_j]
		&= \sum\limits_{i=1}^n \E[(\E_{i=1}[Z - \E^{i}[Z]])^2]\\
		&\leq\sum\limits_{i=1}^n \E[(Z - \E^{(i)}[Z])^2]
	\end{align*}
	which gives us Efron-Stein.  

%------------------------------
%     Lecture 4
%------------------------------

\section{Basic information inequalities}
\label{sec:bas_inf_ineq}
\paragraph{\textbf{Abstract}}
	In this lecture we lay the groundwork for the `entropy method'.
	In particular we discuss basic properties of the Shannon entropy (relation to conditioning and `Chain Rule'),
	we sketch the proofs of Han's inequality and its variant for relative entropies,
	and we prove sub-additivity of the entropy for discrete random variables
	(which is an exact analogue of the Efron-Stein inequality where entropies replace variances).

\begin{theorem}[Sub-Additivity of Entropy]
\label{thm:sub_add_entropy}
	Let $\Phi(x) \defeq x\log x$ for $x>0$, let $X_1,\,\dots,\,X_n$ be independent random variables, and
	let $Y = f(X_1,\,\dots,\,X_n)$ be a nonnegative measurable function of these variables such that $\Phi(Y) = Y\log Y$ is integrable.

	Denote by 
	\begin{itemize}
		\item	$\Ent (Y)$ the \emph{entropy} of $Y$ defined by $$\Ent (Y) \defeq \E\Phi (Y) - \Phi(\E Y),$$ and,
		\item	for every $1\leq i \leq n$, $\Ent^{(i)} (Y)$ the \emph{conditional entropy} of $Y$ given $X^{(i)}$
			defined by $$\Ent^{(i)} \defeq \E^{(i)}\Phi (Y) - \Phi(\E^{(i)} Y).$$
	\end{itemize}

	Then $$\Ent (Y) \leq \E \sum_{i=1}^n \Ent^{(i)} (Y).$$
\end{theorem}

\paragraph{\textbf{Entropy as a Kullback-Leibler divergence (discrete case)}}
\label{rmk:entropy_as_KL_divergence}
	For any discrete probability distributions $P$ and $Q$, if $P$ is absolutely continuous with respect to $Q$ then the Kullback-Leibler divergence from $P$ to $Q$ is defined as
	$$D(P||Q) \defeq \sum_x p(x) \log \frac{p(x)}{q(x)}.$$
	Recall that this can be interpreted as the additional string length required (on average) to encode a string generated by $P$ using a code designed for $Q$.
	This is why $P$ and $Q$ are sometimes referred to as the `reference' and `target' distributions respectively.

	Now consider a random variable $Z=f(X)$ such that $\E Z=1$, let $P$ be the disribution of $X$, and let $Q$ be defined via $q(x) = f(x) p(x)$.
	Then $$D(Q||P) = \Ent Z.$$
	Indeed $$D(Q||P) = \sum_x f(x) p(x) \frac{f(x) p(x)}{p(x)} = \sum_x f(x) p(x) \log f(x) = \E Z Log Z = \Ent Z$$
	since $\Phi(\E Z) = \Phi(1) = 0$.

%------------------------------
%     Lecture 5
%------------------------------

\section{Logarithmic Sobolev inequalities}
\label{sec:log_Sob_ineq}
\paragraph{\textbf{Abstract}}
	The main goal of this chapter is to develop analogues to the Efron-Stein inequality.
	In particular we will see the similarity between the Efron-Stein inequality and the logarithmic Sobolev inequality,
	and between the Gaussian Poincar\'{e} inequality and the Gaussian logarithmic Sobolev inequality.
	In each case, not only do these inequalities have structural similarities, but the latter ones are generalizations of the former ones (in some sense).
	With this similarity in mind, we can generalize the concept of entropy to $\Phi$-entropies, which will be described in Chapter 14.
	Although the powerful inequalities proved in this chapter only apply to a restricted class of random variables, they will be generalized in Chapter 6.

\begin{theorem}[Logarithmic Sobolev inequality for the symmetric Bernoulli distribution]
\label{thm:log_Sob_ineq_sym_Bernoulli}
	Let $f:{\{-1,1\}}^n \rightarrow \R$ be an arbitrary real-valued function defined on the $n$-dimensional binary hypercube ${\{-1,1\}}^n$
	and assume that $X$ is uniformly distributed over ${\{-1,1\}}^n$. Then 
	$$ \Ent(f^2) \leq 2\,\mathcal{E}(f)$$
	where $$\mathcal{E}(f) = \frac{1}{4}\,\E\left[\sum_{i=1}^n \left(f(X)-f(\overline{X}^{(i)})\right)^2\right]
	= \frac{1}{2}\,\E\left[\sum_{i=1}^n \left(f(X)-f(\overline{X}^{(i)})\right)^2_+\right]$$
	and where $$\overline{X}^{(i)} = (X_1,\,\dots,\,X_{i-1},\,-X_i,\,X_{i+1},\,\dots,\,X_n)$$ is obtained by flipping the $i$-th component of $X$ while leaving the others intact.
\end{theorem}

	The central limit theorem kicks in when we extend \fref{Theorem}{thm:log_Sob_ineq_sym_Bernoulli} to a random variable $X$ with a standard normal distribution: 
	for any $\varepsilon_i$ which are uniformly distributed on $\{-1,1\}$ (i.e. Rademacher random variables)
	$$\frac{1}{\sqrt{n}}\sum_{i=1}^n \epsilon_i\rightarrow X$$
	as $n\to\infty$. We thus have:

\begin{theorem}[Gaussian logarithmic Sobolev inequality]
\label{thm:Gaussian_log_Sob_ineq}
	Let $X=(X_1,\,\dots,\,X_n)$ be a vector of $n$ independent standard normal random variables and let $f:\Rn \to \Rn$ be a continuously differentiable function. Then
	$$ \Ent(f^2) \leq 2\Vert \nabla f(X) \Vert^2.$$
\end{theorem}

\paragraph{\textbf{Herbst's argument}}
\label{rmk:herbst}
	Herbst's argument is the main tool which will lead us to concentration inequalities.
	By putting $g_\lambda(x)\defeq e^{\lambda f(x)/2}$ we can derive a differential inequality satsfied by the entropy.
	Solving this inequality leads to a concentration result. To be precise: for $Z = f(x)$,
	$$ \Ent (g_\lambda^2(X)) = \Ent (e^{\lambda f(X)}) = \lambda\E[Ze^{\lambda Z}] - \E e^{\lambda Z}\log\E e^{\lambda Z}.$$
	Let $F(\lambda) = \E e^{\lambda Z}$ so that we can express $\E[Ze^{
	\lambda Z}] = F'(\lambda)$. Therefore
	$$\Ent(g^2_\lambda (X)) = \lambda F'(\lambda) - F(\lambda)\log F(\lambda).$$
	Tsirelson, Ibragimov and Sudakov used Herbst's argument to prove exponential tail inequalites for $L$-Lipschitz functions of independent Gaussian random variables.
	It should be noted that the resulting concentration bounds are dimension-free. This feature allows us to extend the results from previous chapters.

%------------------------------
%     APPENDIX
%------------------------------

\appendix
\newpage
\section*{List of definitions}
	\begin{tabular}{l}
		\hyperref[thm:ESI]{Conditional expectation ($\E^{(i)}$)}\\
		\hyperref[thm:ESI]{Conditional variance ($\var^{(i)}$)}\\
		\hyperref[thm:sub_add_entropy]{Entropy ($\Ent$) and conditional entropy ($\Ent^{(i)}$)}
	\end{tabular}
\section*{List of results}
	\begin{tabular}{l}
		\hyperref[thm:hoeffding]{Hoeffding's inequality}\\
		\hyperref[thm:ESI]{Efron-Stein inequality}\\
		\hyperref[thm:sub_add_entropy]{Sub-additivity of entropy}\\
		\hyperref[thm:log_Sob_ineq_sym_Bernoulli]{Logarithmic Sobolev inequality for the symmetric Bernoulli distribution}\\
		\hyperref[thm:Gaussian_log_Sob_ineq]{Gaussian logarithmic Sobolev inequality}
	\end{tabular}
\section*{List of `tricks'}
	\begin{tabular}{l}
		\hyperref[sec:method_of_isoperimetric_inequalities]{Method of isoperimetric inequalities}\\
		\hyperref[sec:entropy_method]{Entropy method}\\
		\hyperref[sec:transport_method]{Transport method}\\
		\hyperref[rmk:cramer_chernoff]{Cram\'{e}r-Chernoff method}\\
		\hyperref[rmk:Fubini]{Fubini trick}\\
		\hyperref[rmk:entropy_as_KL_divergence]{Entropy as a Kullback-Leibler divergence (discrete case)}\\
		\hyperref[rmk:herbst]{Herbst's argument}
	\end{tabular}
\section*{Credits}
	\begin{tabular}{ll}
		\fref{Lecture}{sec:introduction}& David Gutman\\
		\fref{Lecture}{sec:basic_inequalites}& Adrian Hagerty\\
		\fref{Lecture}{sec:bound_var}& David Itkin\\
		\fref{Lecture}{sec:bas_inf_ineq}& Antoine Remond-Tiedrez\\
		\fref{Lecture}{sec:log_Sob_ineq}& Won Eui Hong
	\end{tabular}

\bibliographystyle{alpha}
\bibliography{ref}
\bibentry{boucheron_lugosi_massart}

\end{document}
