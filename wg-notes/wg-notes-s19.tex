\documentclass[reqno]{amsproc}

%------------------------------
%     PREAMBLE
%------------------------------

% Packages
\usepackage{amsmath, amsthm, hyperref, mathtools} 
\usepackage[usenames, dvipsnames]{xcolor}
\usepackage{bibentry}
\usepackage{lipsum}

% Hyperlink package parameters
\hypersetup{
	colorlinks=true,
	linkcolor=RoyalBlue,
	citecolor=Thistle
}

% amsthm - Declaring theorem commands
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{remark}[theorem]{Remark}

% Referencing including the format name (e.g. 'Section', 'Theorem') not just the numbering
\newcommand{\fref}[2]{\hyperref[#2]{#1 \ref*{#2}}}

% SHORTCUTS
% Notation (general)
\newcommand{\defeq}{\vcentcolon=} % :=
\newcommand{\eqdef}{=\vcentcolon} % =:
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Rn}{\mathbb{R}^n}
% Notation (probability)
\renewcommand{\P}{\mathbb{P}} % Probability
\newcommand{\I}{\mathbb{I}} % Indicator
\newcommand{\E}{\mathbb{E}} % Expectation
\newcommand{\F}{\mathcal{F}} % Filtration
\newcommand{\var}{\mathbb{VAR}} % Variance
\DeclareMathOperator{\Ent}{Ent} % Entropy
\newcommand{\X}{\mathcal{X}}

%------------------------------
%     DOCUMENT
%------------------------------

% Title
\title{Concentration of measure}
\author{SIAM Working Group - Spring 2019}

\begin{document}

% Title and table of contents
\maketitle
\tableofcontents

%------------------------------
%     Lecture 1
%------------------------------

\section{Introduction}
\label{sec:introduction}
\paragraph{\textbf{Abstract}}
	In this lecture, we introduce the book's central theme, the study of random fluctuations of functions of independent random variables, along with three techniques that facilitate this study. The specific focus is on how these functions concentrate around measures of central tendency such as their mean and median. We discuss the three main methods below.

\subsection*{Method 1: Isoperimetric Inequalities}
\label{sec:method_of_isoperimetric_inequalities}
	Suppose $(\mathcal{X},d)$ is a metric space, $X$ is a $\mathcal{X}$-valued RV with law $\P$, and $Mf(X)$ is a median of $f(X)$. Then
	\[
		\P\left\{|f(X)-Mf(X)|\geq t\right\}\leq 2\alpha(t)
	\]
	where
	\[
		\alpha(t)\defeq\sup_{A\in \mathcal{B}(\mathcal{X})\atop \P(A)\geq\frac{1}{2}}\P\{d(X,A)\geq t\}
	\]
	and $\mathcal{B}(\mathcal{X})$ is the Borel $\sigma$-algebra on $\mathcal{X}$. Thus, bounding $\alpha(t)$ allows one to describe how $f(X)$ concentrates around its median.

\subsection*{Method 2: Entropy}
\label{sec:entropy_method}
	Let $I\subseteq\R$ be an interval, $X$ a $I$-valued RV, and $\Phi:I\to\R$ a convex function. then the $\Phi$-entropy of $X$ is
	\[
		H_\Phi(X)\defeq E\Phi(X)-\Phi(EX).
	\]
	If $\Phi(x)=x\log(x)$ then we write $\text{Ent}$ in place of $H_\Phi$. Bounds on this entropy can be translated to bounds on the concentration of functions of random variables around their mean. For example the Gaussian logarithmic Sobolev inequality states that if $X\sim N(0,I_{n\times n})$ and $f\in C^1(\Rn)$ then
	\[
		\text{Ent}\left[f^2(X)\right]\leq 2E\left[\|\nabla f(X)\|^2\right].
	\]
	In turn, this implies that
	\[
		\P\left\{f(X)-Ef(X)\geq t\right\}\leq e^{-\frac{t^2}{2}}.
	\]

\subsection*{Method 3: Transportation}
\label{sec:transport_method}
	Another way of deriving concentration bounds is by getting estimates of the \textit{transportation cost} between two probability measures $P$ and $Q$:
	\[
		\min_{\mathbf{P}\in\mathcal{P}(P,Q)} \E_\mathbf{P}d(X,Y)
	\]
	where $d$ is some cost function, and $\mathcal{P}(P,Q)$ is the class of joint distributions of $X$ and $Y$ such that the marginal distribution of $X$ is $P$ and the marginal distribution of $Y$ is $Q$.

%------------------------------
%     Lecture 2
%------------------------------

\section{Basic inequalities}
\label{sec:basic_inequalites}
\paragraph{\textbf{Abstract}}
	We use the Cram\'{e}r-Chernoff method to establish a family of inequalities which give quantitative estimates on concentration of measure subject to certain regularity conditions. We use the machinery of the Cram\'{e}r-Chernoff method, the log of the moment generating function, to define families of random variables with rapidly decaying tails, sub-Gaussian and more generally sub-Gamma random variables.

\begin{theorem}[Hoeffdinig's Inequality]
\label{thm:hoeffding}
	Let $X_1,\, \dots,\, X_n$ be independent random variables such that $X_i$ takes its values in $[a_i, b_i]$ almost surely for all $i \leq n$. Let
	\[S \defeq \sum_{i=1}^n (X_i - \E X_i).\]
	Then for every $t > 0$,
	\[\P \{S \geq t \} \leq  \exp \left( \frac{-2t^2}{\sum_{i=1}^n (b_i-a_i)^2} \right).\]
\end{theorem}

\paragraph{\textbf{Cram\'{e}-Chernoff Method}}
\label{rmk:cramer_chernoff}
	For a random variable $X$, we use the fact that $\exp(\cdot)$ is an increasing function along with Markov's inequality to get the estimate
	\[\P \{X \geq t \} = \P \{e^{\lambda X} \geq e^{\lambda t} \} \leq e^{-\lambda t} \E \left( e^{\lambda X} \right)\]
	for every $\lambda > 0$.
	Picking the optimal $\lambda$ for a fixed $t$ yields
	\[\P \{X \geq t \} \leq \exp \left( -\psi^*(t) \right) \]
	where 
	\[\psi(\lambda) \defeq \log \E \left( e^{\lambda X} \right) \text{ and}\]
	\[\psi^*(t) \defeq \sup_{\lambda > 0} \left( t \lambda - \psi(\lambda) \right) \]
	which agrees with the Legendre transform of $\psi$ when $t > \E X$.

%------------------------------
%     Lecture 3
%------------------------------

\section{Bounding the variance}
\label{sec:bound_var}
\paragraph{\textbf{Abstract}}
We extensively utilize the \textit{Efron-Stein} inequality, which gives a bound on the variance of random variables in terms of the mean of their conditional variances. This simple bound has surprising consequences when applied to specific types of functions of random variables and allows us to almost effortlessly obtain sharp bounds on the variance. This is particularly the case for functions satisfying the \textit{bounded difference}, \emph{self-bounding} and \emph{configuration} properties. Other applications include proving Poincar\'{e} type inequalities for Gaussian RV's and deriving exponential tail bounds.

\begin{theorem}[Efron-Stein Inequality]
\label{thm:ESI}
	Let $X_1,\,\dots,\,X_n$ be independent random variables taking values in $\mathcal{X}$ and let $f: \mathcal{X}^n \to \R$ be given.
	Set $Z = f(X_1,\,\dots,\,X_n)$ and assume $Z \in L^2$. Then we have that 
	$$\var(Z)\leq \sum\limits_{i=1}^n \E[(Z-\E^{(i)}[Z])^2] \eqdef \sum\limits_{i=1}^n \E[\var^{(i)}[Z]]$$ where 
	$$\E^{(i)}[Z] \defeq \E[Z|X_1,\,\dots,\,X_{i-1},\,X_{i+1},\,\dots,\,X_n]$$
	i.e. $\E^{(i)}$ the expectation operator conditioned on $$X^{(i)} = (X_1,\,\dots,\,X_{i-1},\,X_{i+1},\,\dots,\,X_n),$$
	and $$\var^{i}[Z] \defeq \E^{(i)}[(Z - \E^{(i)}[Z])^2].$$
\end{theorem}

\paragraph{\textbf{Fubini Trick}}
\label{rmk:Fubini}
	If we write $Z = f(X_1,\,\dots,\,X_n)$ as before and 
	$\E_i[Z] \defeq \E[Z|X_1,\,\dots,\,X_i]$ then by Fubini we have that 
	$$\E_i[Z] = \int_{\mathcal{X}^{n-i}} f(X_1,\,\dots,\,X_i,\,x_{i+1},\,\dots,\,x_{n})d\mu_{i+1}(x_{i+1})\dots d\mu_n(x_{n})$$ and similarly
	$$\E^{(i)}[Z] = \int_\mathcal{X} f(X_1,\,\dots,\,X_{i-1},\,x_i,\,X_{i+1},\,\dots,\,X_n)d\mu_i(x_i)$$ so that we have the identity
	$$\E_i[\E^{(i)}[Z]] = \E_{i-1}[Z], \hspace{0.3 cm} \forall i \geq 1$$
	where we use the convention that $\E_0[Z] = \E[Z]$. Now if we let $\Delta_i \defeq \E_i[Z] - \E_{i-1}[Z]$ then by this Fubini observation it follows that 
	$$Z - \E[Z] = \sum\limits_{i=1}^n \Delta_i = \sum\limits_{i=1}^n \E^i[Z - E^{(i)}[Z]]$$ 
	Since for $j > i$ we have that
	$$\E[\Delta_i\Delta_j] = \E[\Delta_i\E_i[\Delta_j]] = 0$$ it follows that, using Jensen's inequality,
	\begin{align*}
		\var(Z)
		= \E[(Z-\E[Z])^2]
		= \sum\limits_{i=1}^n \E[\Delta_i^2] + 2\sum\limits_{j>i}\E[\Delta_i\Delta_j]
		&= \sum\limits_{i=1}^n \E[(\E_{i=1}[Z - \E^{i}[Z]])^2]\\
		&\leq\sum\limits_{i=1}^n \E[(Z - \E^{(i)}[Z])^2]
	\end{align*}
	which gives us Efron-Stein.  

%------------------------------
%     Lecture 4
%------------------------------

\section{Basic information inequalities}
\label{sec:bas_inf_ineq}
\paragraph{\textbf{Abstract}}
	In this lecture we lay the groundwork for the `entropy method'.
	In particular we discuss basic properties of the Shannon entropy (relation to conditioning and `Chain Rule'),
	we sketch the proofs of Han's inequality and its variant for relative entropies,
	and we prove sub-additivity of the entropy for discrete random variables
	(which is an exact analogue of the Efron-Stein inequality where entropies replace variances).

\begin{theorem}[Sub-Additivity of Entropy]
\label{thm:sub_add_entropy}
	Let $\Phi(x) \defeq x\log x$ for $x>0$, let $X_1,\,\dots,\,X_n$ be independent random variables, and
	let $Y = f(X_1,\,\dots,\,X_n)$ be a nonnegative measurable function of these variables such that $\Phi(Y) = Y\log Y$ is integrable.

	Denote by 
	\begin{itemize}
		\item	$\Ent (Y)$ the \emph{entropy} of $Y$ defined by $$\Ent (Y) \defeq \E\Phi (Y) - \Phi(\E Y),$$ and,
		\item	for every $1\leq i \leq n$, $\Ent^{(i)} (Y)$ the \emph{conditional entropy} of $Y$ given $X^{(i)}$
			defined by $$\Ent^{(i)} \defeq \E^{(i)}\Phi (Y) - \Phi(\E^{(i)} Y).$$
	\end{itemize}

	Then $$\Ent (Y) \leq \E \sum_{i=1}^n \Ent^{(i)} (Y).$$
\end{theorem}

\paragraph{\textbf{Entropy as a Kullback-Leibler divergence (discrete case)}}
\label{rmk:entropy_as_KL_divergence}
	For any discrete probability distributions $P$ and $Q$, if $P$ is absolutely continuous with respect to $Q$ then the Kullback-Leibler divergence from $P$ to $Q$ is defined as
	$$D(P||Q) \defeq \sum_x p(x) \log \frac{p(x)}{q(x)}.$$
	Recall that this can be interpreted as the additional string length required (on average) to encode a string generated by $P$ using a code designed for $Q$.
	This is why $P$ and $Q$ are sometimes referred to as the `reference' and `target' distributions respectively.

	Now consider a random variable $Z=f(X)$ such that $\E Z=1$, let $P$ be the disribution of $X$, and let $Q$ be defined via $q(x) = f(x) p(x)$.
	Then $$D(Q||P) = \Ent Z.$$
	Indeed $$D(Q||P) = \sum_x f(x) p(x) \frac{f(x) p(x)}{p(x)} = \sum_x f(x) p(x) \log f(x) = \E Z Log Z = \Ent Z$$
	since $\Phi(\E Z) = \Phi(1) = 0$.

%------------------------------
%     Lecture 5
%------------------------------

\section{Logarithmic Sobolev inequalities}
\label{sec:log_Sob_ineq}
\paragraph{\textbf{Abstract}}
	The main goal of this chapter is to develop analogues to the Efron-Stein inequality.
	In particular we will see the similarity between the Efron-Stein inequality and the logarithmic Sobolev inequality,
	and between the Gaussian Poincar\'{e} inequality and the Gaussian logarithmic Sobolev inequality.
	In each case, not only do these inequalities have structural similarities, but the latter ones are generalizations of the former ones (in some sense).
	With this similarity in mind, we can generalize the concept of entropy to $\Phi$-entropies, which will be described in Chapter 14.
	Although the powerful inequalities proved in this chapter only apply to a restricted class of random variables, they will be generalized in Chapter 6.

\begin{theorem}[Logarithmic Sobolev inequality for the symmetric Bernoulli distribution]
\label{thm:log_Sob_ineq_sym_Bernoulli}
	Let $f:{\{-1,1\}}^n \rightarrow \R$ be an arbitrary real-valued function defined on the $n$-dimensional binary hypercube ${\{-1,1\}}^n$
	and assume that $X$ is uniformly distributed over ${\{-1,1\}}^n$. Then 
	$$ \Ent(f^2) \leq 2\,\mathcal{E}(f)$$
	where $$\mathcal{E}(f) = \frac{1}{4}\,\E\left[\sum_{i=1}^n \left(f(X)-f(\overline{X}^{(i)})\right)^2\right]
	= \frac{1}{2}\,\E\left[\sum_{i=1}^n \left(f(X)-f(\overline{X}^{(i)})\right)^2_+\right]$$
	and where $$\overline{X}^{(i)} = (X_1,\,\dots,\,X_{i-1},\,-X_i,\,X_{i+1},\,\dots,\,X_n)$$ is obtained by flipping the $i$-th component of $X$ while leaving the others intact.
\end{theorem}

	The central limit theorem kicks in when we extend \fref{Theorem}{thm:log_Sob_ineq_sym_Bernoulli} to a random variable $X$ with a standard normal distribution: 
	for any $\varepsilon_i$ which are uniformly distributed on $\{-1,1\}$ (i.e. Rademacher random variables)
	$$\frac{1}{\sqrt{n}}\sum_{i=1}^n \epsilon_i\rightarrow X$$
	as $n\to\infty$. We thus have:

\begin{theorem}[Gaussian logarithmic Sobolev inequality]
\label{thm:Gaussian_log_Sob_ineq}
	Let $X=(X_1,\,\dots,\,X_n)$ be a vector of $n$ independent standard normal random variables and let $f:\Rn \to \Rn$ be a continuously differentiable function. Then
	$$ \Ent(f^2) \leq 2\Vert \nabla f(X) \Vert^2.$$
\end{theorem}

\paragraph{\textbf{Herbst's argument}}
\label{rmk:herbst_1}
	Herbst's argument is the main tool which will lead us to concentration inequalities.
	By putting $g_\lambda(x)\defeq e^{\lambda f(x)/2}$ we can derive a differential inequality satsfied by the entropy.
	Solving this inequality leads to a concentration result. To be precise: for $Z = f(x)$,
	$$ \Ent (g_\lambda^2(X)) = \Ent (e^{\lambda f(X)}) = \lambda\E[Ze^{\lambda Z}] - \E e^{\lambda Z}\log\E e^{\lambda Z}.$$
	Let $F(\lambda) = \E e^{\lambda Z}$ so that we can express $\E[Ze^{
	\lambda Z}] = F'(\lambda)$. Therefore
	$$\Ent(g^2_\lambda (X)) = \lambda F'(\lambda) - F(\lambda)\log F(\lambda).$$
	Tsirelson, Ibragimov and Sudakov used Herbst's argument to prove exponential tail inequalites for $L$-Lipschitz functions of independent Gaussian random variables.
	It should be noted that the resulting concentration bounds are dimension-free. This feature allows us to extend the results from previous chapters.

%------------------------------
%     Lectures 6 & 7
%------------------------------

\section{Entropy method}
\label{sec:EntropyMethod}

\paragraph{\textbf{Abstract}}
	In this chapter, we will explore a method of proof called the \emph{entropy method}.
	The main idea is that one will try to use the convexity of the entropy and Herbst's argument in order to prove the desired inequalities.
	Important results obtained via this method include an exponential inequality for self-bounded functions and the exponential Efron-Stein inequality.

\paragraph{\textbf{Herbst's agument}}
\label{rmk:herbst_2}
	Let $Z = f(X_1,\dots,X_n)$ where the $X_i$'s are independent random variables (not necessarily identically distributed).
	Denoting $\varphi(\lambda)~\defeq~\log \E e^{\lambda(Z - \E Z)}$ one can see that 
	\begin{equation*}
		\frac{ \Ent (e^{\lambda Z})}{ \E e^{\lambda Z}} = \lambda \varphi'(\lambda) -
		\varphi(\lambda) \,.
	\end{equation*}
	Herbst's argument allows us to use an differential inequality to prove that
	if 
	\begin{equation*}
		\frac{ \Ent (e^{\lambda Z})}{ \E e^{\lambda Z}} \leq C\lambda^2 \,,
	\end{equation*}
	then 
	\begin{equation*}
		\log \E e^{\lambda(Z- \E Z)} \leq C\lambda^2 \,.
	\end{equation*}
	Indeed, if
	\begin{equation*}
		\frac{ \Ent (e^{\lambda Z})}{ \E e^{\lambda Z}} \leq C\lambda^2 \,,
	\end{equation*}
	then
	\begin{equation*}
		\lambda \varphi' \left( \lambda \right) - \varphi \left( \lambda \right) = 
		\frac{ \Ent (e^{\lambda Z})}{ \E e^{\lambda Z}} \leq C\lambda^2 \,
	\end{equation*}
	and hence
	\begin{equation*}
		\left( \frac{\varphi \left( \lambda \right)}{\lambda} \right)' = \frac{\varphi' \left( \lambda \right)}{\lambda} - \frac{ \varphi \left( \lambda \right)}{\lambda^2} \leq C^2 .
	\end{equation*}
	Integrating this inequality gives us that $\log \E e^{\lambda(Z - \E Z)} = \varphi \left( \lambda \right) \leq C\lambda^2$.

\paragraph{\textbf{The entropy method}}
\label{rmk:entropy_method}
	In this chapter, a central theme is that one can deduce from the convexity (or sub-additivity) of the entropy a modified logarithmic Sobolev inequality:
	\begin{equation*}
		\Ent \left( e^{\lambda Z} \right) = \lambda \E ( Ze^{\lambda Z}) - \E( e^{\lambda Z})\log \E(e^{\lambda Z}) \leq
		\sum_{i=1}^n \E (e^{\lambda Z} \phi( - \lambda (Z-Z_i))) \,,
	\end{equation*}
	where $\phi (x) \defeq e^x - x - 1$ and $Z_i \defeq f \left( X^{(i)} \right) = f \left( X_1,\,\dots,\,X_{i-1},X_{i+1},\dots,\,X_n \right).$
	Combining this with Herbst's argument allows us to prove all sorts of exponential inequalities.

\begin{theorem}[Exponential self-bounding inequality]
	\label{thm:exp_self_bound_ineq}
	Let $X_1,\,\dots,\,X_n$ be independent random variables taking values in $ \mathcal{X} $ and let $f: \mathcal{X} \to \left[0,\infty\right)$ satisfy the self-bounding property,
	i.e. there exist functions $f_i : \mathcal{X}^{n-1} \to \R$ such that, for all $x_1,\,\dots,\,x_n\in \mathcal{X} $ and all $i=1,\,\dots,\,n$,
	\begin{equation*}
		0 \leq f \left( x_1,\,\dots,\,x_n \right) - f_i \left( x_1,\,\dots,\,x_{i-1},\,x_{i+1},\,\dots,\,x_n \right) \leq 1
	\end{equation*}
	and
	\begin{equation*}
		\sum_{i=1}^n \left(
			f \left( x_1,\,\dots,\,x_n \right) - f_i \left( x_1,\,\dots,\,x_{i-1},\,x_{i+1},\,\dots,\,x_n \right)
		\right) \leq f \left( x_1,\,\dots,\,x_n \right).
	\end{equation*}
	Then, for any $\lambda\in\R$ and for $Z \defeq f \left( X_1,\,\dots,\,X_n \right)$,
	\begin{equation*}
		\log \E e^{\lambda \left( Z-\E Z \right)} \leq \phi(\lambda) \E Z\,.
	\end{equation*}
	where $\phi(x) \defeq e^x - x - 1$.
\end{theorem}
	For example, $f = {||\cdot||}_{l^p}^p$ is a self-bounding function on ${ \left[ 0,1 \right] }^n$ for any $1\leq p < \infty$ since 
		$f-f_i = {|x_i|}^p \in \left[ 0,1 \right]$
	and
		$\sum_i \left( f-f_i \right) = \sum_i {|x|}^i = {|x_i|}^p = f$.
	Similarly, on any bounded set, appropriate multiples of ${||\cdot||}_{l^p}^p$ are self-bounding functions.
\begin{theorem}[Exponential Efron-Stein inequality]
	\label{thm:exp_efron_stein}
	Let $X_1,\,\dots,\,X_n$ be independent random variables and let $Z = f \left( X_1,\,\dots,\,X_n \right)$.
	For each $i=1,\,\dots,\,n$ let $X_i'$ be an independent copy of $X_i$ and let $X_i' \defeq f \left( X_1,\,\dots,\,X_i',\,\dots,\,X_n \right)$.
	Define $\E' \defeq \E \left[ \,\cdot\, | X \right]$ (i.e. integrating out the variables $X_1',\,\dots,\,X_n'$) and
	\begin{equation*}
		V^+ \defeq \sum_{i=1}^n \E' \left[ {\left( Z - Z_i' \right)}_+^2 \right]
	\end{equation*}
	and let $\theta, \lambda > 0$ such that $\theta\lambda < 1$ and $\E e^{\lambda V^+ / \theta} < \infty$. Then
	\begin{equation*}
		\log\E e^{\lambda \left( Z - \E Z \right)} \leq \frac{\lambda\theta}{1-\lambda\theta} \log\E e^{\lambda V^+ / \theta}\,.
	\end{equation*}
\end{theorem}

%------------------------------
%     Lectures 8 & 9
%------------------------------

\section{Concentration and Isoperimetry}
\label{sec:iso}
\paragraph{\textbf{Abstract}}
	In this chapter, we discuss the relationship between isoperimeter problems and concentration of measure.
	In particular, we introduce the concentration function and L\'evy's inequalities.
	We also discuss Talagrand's convex distance inequality.
	The key to this proof involves the idea of a self bounding function, and that the squared convex distance is self bounding.

\begin{theorem}[L\'evy's inequalities]
\label{thm:levy}
	Given a metric space $(\X, d)$ and a random variable $X$ on $\X$ distributed according to the measure $\mu$, we define the concentration function
	\[\alpha(t) := \sup_{A \subset \X, \ \mu(A) \geq 1/2} \P \left\{ d(X,A) \geq t \right \} \]
	For any 1-Lipschitz function $f: \X \to \R$, we have
	\[\P \{f(X) \geq Mf(X) + t \} \leq \alpha(t) \textrm{ and } \P \{f(X) \leq Mf(X) - t \} \leq \alpha(t), \]
	where $Mf(X)$ denotes any median of $f(X)$. Conversely, if $\beta : (0,\infty) \to [0,1]$ is a function such that for every 1-Lipschitz function $f : \X \to \R$ we have 
	\[ \P \left\{f(X) \geq Mf(X) + t \right\} \leq \beta(t),\]
	then $\beta(t) \leq \alpha(t)$.
\end{theorem}

\begin{theorem}[Talagrand's convex distance inequality]
\label{thm:tal}
	Consider independent random variables $X_1,\, \dots,\, X_n$ taking values in $\X$ and denote their vector as $X = (X_1, \dots, X_n) \in \X^n$.
	For $\alpha \in [0, \infty)^n$ we define the weighted Hamming distance to a set $A \subset \X^n$ by
	\[d_{\alpha}(x, A) = \inf_{y \in A} \sum_{i: x_i \not = y_i} |\alpha_i| \]
	The convex distance to a set $A$ is given by
	\[d_T(x, A) := \sup_{ \alpha \in [0, \infty)^n, \|\alpha\| = 1} d_{\alpha}(x,A)\]
	where $\|\cdot\|$ denotes the Euclidean norm.
	We have
	\[\P(A)\,\P(d_T(X,A) \geq t) \leq e^{-t^2/4}.\]
\end{theorem}

\paragraph{ \bf Gaussian isoperimetric inequality} \label{rmk:iso}
	L\'evy's inequalities provide us with the connection between concentration of measure type results and isoperimetric problems.
	In particular, we can get isoperimetric results very cheaply by using our concentration theorems.
	We will demonstrate a way to get a very sharp isoperimetric inequality for the Gaussian measure.

	If $X$ is a standard Gaussian vector then for any 1-Lipschitz $f: \R^n \to \R$ and $t > 0$,
	\begin{align*}
	\P \{ f(X) \geq Mf(X) + t \} &= \P \{ f(X) \geq Ef(X) + (Mf(X) - Ef(X) + t) \} \\
	& \leq \exp \bigg\{ \frac{1}{2}(Mf(X) - Ef(X) + t)^2\bigg\},
	\end{align*}
	where we have used a concentration result for $f(X)$. Now, for any random variable $Z$ we have
	\[|MZ - EZ| \leq \sqrt{\var(Z)},\]
	so in particular we have
	\[ Mf(X) - Ef(X) \leq \sqrt{\var(f(X))} \leq 1\]
	and so, by L\'evy's inequality,
	\[ \alpha(t) \leq e^{-\frac{1}{2}(1+t)^2}. \]

%------------------------------
%     APPENDIX
%------------------------------

\appendix
\newpage


\section*{List of definitions}
	\begin{tabular}{l}
		\hyperref[thm:ESI]{Conditional expectation ($\E^{(i)}$)}\\
		\hyperref[thm:ESI]{Conditional variance ($\var^{(i)}$)}\\
		\hyperref[thm:sub_add_entropy]{Entropy ($\Ent$) and conditional entropy ($\Ent^{(i)}$)}\\
		\hyperref[thm:exp_self_bound_ineq]{Self-bounding function}
	\end{tabular}
\section*{List of results}
	\begin{tabular}{l}
		\hyperref[thm:hoeffding]{Hoeffding's inequality}\\
		\hyperref[thm:ESI]{Efron-Stein inequality}\\
		\hyperref[thm:sub_add_entropy]{Sub-additivity of entropy}\\
		\hyperref[thm:log_Sob_ineq_sym_Bernoulli]{Logarithmic Sobolev inequality for the symmetric Bernoulli distribution}\\
		\hyperref[thm:Gaussian_log_Sob_ineq]{Gaussian logarithmic Sobolev inequality}\\
		\hyperref[thm:sub_add_entropy]{Exponential self-bounding inequality}\\
		\hyperref[thm:exp_efron_stein]{Exponential Efron-Stein inequality}\\
		\hyperref[thm:levy]{L\'evy's inequalities}\\
		\hyperref[thm:tal]{Talagrand's convex distance inequality}
	\end{tabular}
\section*{List of `tricks'}
	\begin{tabular}{l}
		\hyperref[sec:method_of_isoperimetric_inequalities]{Method of isoperimetric inequalities}\\
		Entropy method (\hyperref[sec:entropy_method]{part 1}, \hyperref[rmk:entropy_method]{part 2})\\
		\hyperref[sec:entropy_method]{Entropy method}\\
		\hyperref[sec:transport_method]{Transport method}\\
		\hyperref[rmk:cramer_chernoff]{Cram\'{e}r-Chernoff method}\\
		\hyperref[rmk:Fubini]{Fubini trick}\\
		\hyperref[rmk:entropy_as_KL_divergence]{Entropy as a Kullback-Leibler divergence (discrete case)}\\
		Herbst's argument (\hyperref[rmk:herbst_1]{part 1}, \hyperref[rmk:herbst_2]{part 2})\\
		\hyperref[rmk:iso]{Gaussian isoperimetric inequality}
	\end{tabular}
\section*{Credits}
	\begin{tabular}{ll}
		\fref{Lecture}{sec:introduction}& David Gutman\\
		\fref{Lecture}{sec:basic_inequalites}& Adrian Hagerty\\
		\fref{Lecture}{sec:bound_var}& David Itkin\\
		\fref{Lecture}{sec:bas_inf_ineq}& Antoine Remond-Tiedrez\\
		\fref{Lecture}{sec:log_Sob_ineq}& Won Eui Hong \\
		\fref{Lecture}{sec:EntropyMethod}& Son Van \\
		\fref{Lecture}{sec:iso}& Adrian Hagerty \\
		Editing& Antoine Remond-Tiedrez
	\end{tabular}

\bibliographystyle{alpha}
\bibliography{ref}
\bibentry{boucheron_lugosi_massart}

\end{document}
